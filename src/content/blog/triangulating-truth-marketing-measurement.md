---
title: "Triangulating Truth: A Model for Marketing Measurement"
subtitle: "No single measurement methodology tells the truth. But three of them together get close."
date: 2025-11-20
description: "Attribution models lie. So do surveys and incrementality tests. But by triangulating across all three, marketers can get closer to the truth about what's actually working."
---

Here's a dirty secret about marketing measurement: every methodology lies. Attribution models over-credit the last touch. Media mix models are backward-looking and coarse. Survey-based approaches are biased by memory and social desirability. Incrementality tests are expensive and narrow in scope.

And yet, marketing leaders are expected to make multi-million dollar allocation decisions based on these tools. The question isn't which methodology to trust — it's how to use multiple imperfect signals to approximate something closer to truth.

I call this approach Triangulating Truth, and after years of refining it, I believe it's the most honest framework available for marketing measurement.

## The three vertices

Triangulation requires at least three independent measurement approaches. Each has different strengths, different weaknesses, and different blind spots. The signal lives in the convergence.

### Vertex 1: Algorithmic attribution

This includes multi-touch attribution (MTA), last-click, and other model-based approaches that assign credit to touchpoints in the customer journey.

**What it's good at:** Showing relative channel performance within digital. Identifying which touchpoints appear most frequently in conversion paths. Providing fast, granular feedback for tactical optimization.

**Where it lies:** It fundamentally cannot measure incrementality. It mistakes correlation for causation. It's blind to offline influence. And it systematically over-credits channels that appear late in the journey (search, retargeting) while under-crediting channels that create demand (brand, content, social).

### Vertex 2: Experimentation and incrementality

Controlled experiments — holdout tests, geo-matched markets, on/off tests — that measure the causal impact of marketing activity.

**What it's good at:** Answering the question "what would have happened if we hadn't done this?" This is the gold standard for proving that marketing actually caused an outcome rather than just being present when it happened.

**Where it lies:** It's expensive, slow, and narrow. You can test one channel at a time, in one market, for one time period. The results don't always generalize. And the organizational patience required to run clean experiments is rare.

### Vertex 3: Econometric modeling

Marketing mix modeling (MMM), regression analysis, and other statistical approaches that model the relationship between marketing inputs and business outcomes over time.

**What it's good at:** Big-picture allocation guidance. Understanding the relative contribution of channels over months and years. Accounting for external factors (seasonality, competition, economic conditions) that attribution ignores.

**Where it lies:** It's backward-looking. It requires significant historical data. It treats channels as independent when they're deeply interdependent. And the confidence intervals are often wider than marketers want to admit.

## How triangulation works

The magic happens when you overlay all three:

1. **Run your attribution model continuously.** Use it for day-to-day tactical decisions — which keywords to bid on, which creative to scale, where to shift budget within a channel.

2. **Run incrementality tests quarterly.** Pick your highest-spend or most-questioned channels. Design clean holdout experiments. Use the results to calibrate your attribution model — if attribution says paid social drives 30% of conversions but your incrementality test shows only 15% lift, you know the attribution model is double-counting.

3. **Update your econometric model semi-annually.** Use 2-3 years of data to model the relationship between spend and outcomes at the channel level. Compare the allocation recommendations to what attribution and incrementality suggest.

4. **Look for convergence — and investigate divergence.** When all three methodologies point in the same direction, you can be relatively confident. When they disagree, that's where the interesting questions are.

> The goal of measurement isn't certainty. It's calibrated confidence. You'll never know exactly what's working. But you can know enough to make decisions that are better than guessing — and that's all you need.

## A practical example

Suppose your attribution model says email is your best-performing channel (lowest CPA, highest ROAS). Your incrementality test shows that 80% of email-attributed conversions would have happened anyway — those customers were already going to buy. And your econometric model suggests that brand awareness (driven by video and content) is the real driver of demand, with email simply capturing it.

Three different answers. One truth: email is a valuable *conversion* channel but not a valuable *demand* channel. Your attribution model was right about the "what" but wrong about the "why." Without triangulation, you might have doubled your email budget while cutting the brand investment that actually fills the funnel.

## What you need to get started

You don't need perfect data or a massive analytics team. You need:

- **A functioning attribution model.** Even last-click is better than nothing as a starting point.
- **The discipline to run one incrementality test per quarter.** Start with your biggest channel. Keep the methodology simple.
- **An econometric model, even a basic one.** A regression of spend vs. outcomes by channel, controlling for seasonality, gets you 80% of the value of a sophisticated MMM.
- **A culture that values truth over comfort.** This is the hardest part. Triangulation will reveal uncomfortable truths about pet channels and legacy strategies. Leadership must be willing to act on what the data shows, even when it contradicts intuition.

## The meta-lesson

Marketing measurement is not a technology problem. It's an epistemology problem. We're trying to understand a complex, dynamic system with imperfect tools. The organizations that accept this — that embrace uncertainty and build frameworks for navigating it — will consistently make better decisions than those chasing the mirage of perfect attribution.

No single methodology tells the truth. But three of them together get close enough.
